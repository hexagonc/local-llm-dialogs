{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b0e5502-d8d7-4341-8cdb-7f48c43f6762",
   "metadata": {},
   "source": [
    "# LMM Developer Notebook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122c2dc4-9b56-46fa-80e5-b02b2808f044",
   "metadata": {},
   "source": [
    "## Full LLM Dialog Runner in a notebook\n",
    "Demonstrate usage of dialog runner in a Jupyter cell\n",
    "\n",
    "Contains a combination of commands to the sytem as well as commands to the assistant.\n",
    "\n",
    "### System commands:\n",
    "##### Switching to a different dialog branch:\n",
    "`system: switch to branch {new_dialog_branch_filename.json}`\n",
    "The `new_dialog_branch_filename` should be the absolute path to a file that \n",
    "\n",
    "##### Importing the history from a different dialog branch:\n",
    "`system: import branch {existing_dialog_filename.json}`\n",
    "If `{existing_dialog_filename.json}` doesn't exist then an error will be raised\n",
    "\n",
    "##### Popping dialog\n",
    "`system: pop`\n",
    "\n",
    "##### Evaluating assistant shell commands\n",
    "`system: run shell command [(command index)]`\n",
    "Runs the nth assistant shell command from assistant's output.  Shell commands are assistant output delimited by /* */.  Those commands\n",
    "will be run a shell and results will be automatically communicated to the assistant.  The system will impersonate the user and return one of:\n",
    "```\n",
    "stdout:\n",
    "{stdout result from shell command}\n",
    "```\n",
    "or \n",
    "```\n",
    "stderr:\n",
    "{stderr result from shell command}\n",
    "```\n",
    "or if the shell commands has neither a stdout nor stderr then just return the returncode:\n",
    "```\n",
    "returncode: {code}\n",
    "```\n",
    "\n",
    "##### Replay dialog history\n",
    "`system: replay [n]`\n",
    "Displays the last *n* steps of the current dialog.  This function is useful for providing context in the event that you need to refresh your memory for what you were talking about with the LLM when returning to the dialog from another session (which won't have your dialog history by default).\n",
    "\n",
    "##### Introduce a file and its contents into the assistant's awareness\n",
    "`system: introduce the file: {full file path}`\n",
    "\n",
    "##### Show the content the assistant intends to be written to the current file that has been introduced\n",
    "`system: show assistant content`\n",
    "\n",
    "\n",
    "##### Write the assistant's content to a file\n",
    "`system: write assistant content to {name of output file}`\n",
    "\n",
    "##### Dynamically changing assistant model\n",
    "`system: use model name [llama3 | openai]` Using openai will use gpt4o\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d591e6-5f10-4524-acde-3cb086ac3b70",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "import LLMDialogController\n",
    "importlib.reload(LLMDialogController)\n",
    "\n",
    "##########################################\n",
    "##  Main Chat Interface Loop\n",
    "## Call run this cell to converse with the default configuration.  This assumes you are running LM Studio in server mode and are \n",
    "## running a LLama3 model with model identifier: lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF/Meta-Llama-3-8B-Instruct-Q4_K_M.gguf\n",
    "## You also need to be running the embedding model: nomic-ai/nomic-embed-text-v1.5-GGUF/nomic-embed-text-v1.5.Q8_0.gguf\n",
    "\n",
    "\n",
    "\n",
    "base_dialog_path = \".\"\n",
    "user_input = f\"system: import dialog_fs_actions_viewer_and_copying.json\"\n",
    "    \n",
    "print(base_dialog_path)\n",
    "dialog_controller = LLMDialogController.LLMDialogController(dialog_index_path = base_dialog_path)\n",
    "resp = dialog_controller.chat(user_input, contWithStd = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce1c68c-c288-45cf-ab08-812cd90b944d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
